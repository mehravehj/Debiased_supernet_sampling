{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOO/ldLJNFdrrWFZ0DfpQlT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehravehj/Debiased_supernet_sampling/blob/main/Pooling_post_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post process training\n",
        "\n",
        "1.   Load Saved checkpoint (given test number, epoch)\n",
        "2.   get output, feature maps, grads on validation mini-batch\n",
        "3.   Save them in separate folders\n",
        "\n"
      ],
      "metadata": {
        "id": "E_WUbLU6CnP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definitions"
      ],
      "metadata": {
        "id": "YyFl8r0c8ySu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')#, force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioqNK7yGJ48z",
        "outputId": "947f98f1-4c31-49fa-8f54-14b0a9a838b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: All Definitions (Complete with modified ResNet20)\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random # For shuffle in data_loader\n",
        "from datetime import datetime\n",
        "from itertools import combinations\n",
        "from typing import Optional, Tuple, List, Dict, Union, Type # Added Type for block_type hint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "\n",
        "# --- From utility_functions.py ---\n",
        "def string_to_list(x: str, leng: int) -> List[int]:\n",
        "    \"\"\"\n",
        "    Converts a string to a list of integers.\n",
        "    If 'x' is a comma-separated string of numbers, it splits them.\n",
        "    The resulting list must have a length equal to 'leng'.\n",
        "    If 'x' is a single number, it creates a list of 'leng' repetitions of that number.\n",
        "    \"\"\"\n",
        "    if not isinstance(x, str):\n",
        "        raise TypeError(f\"Input 'x' must be a string, got {type(x)}\")\n",
        "    if not isinstance(leng, int) or leng <= 0:\n",
        "        raise ValueError(f\"'leng' must be a positive integer, got {leng}\")\n",
        "\n",
        "    if ',' in x:\n",
        "        parts = x.split(',')\n",
        "        try:\n",
        "            res = [int(p.strip()) for p in parts]\n",
        "        except ValueError as e:\n",
        "            raise ValueError(f\"Invalid number in comma-separated string '{x}': {e}\")\n",
        "        if len(res) != leng:\n",
        "            raise ValueError(f\"Channel string '{x}' provides {len(res)} values, but network depth 'leng' is {leng}. They must match.\")\n",
        "    else:\n",
        "        try:\n",
        "            val = int(x.strip())\n",
        "            res = [val for _ in range(leng)]\n",
        "        except ValueError:\n",
        "            raise ValueError(f\"Channel string '{x}' is not a valid single integer or a comma-separated list of integers.\")\n",
        "    return res\n",
        "\n",
        "# --- From search_space_design.py ---\n",
        "def create_search_space(num_layers: int, num_scales: int) -> Tuple[tuple, int]:\n",
        "    if num_layers < 0: # Allow num_layers = 0 for empty search space\n",
        "        raise ValueError(\"num_layers cannot be negative.\")\n",
        "    if num_scales <= 0:\n",
        "        raise ValueError(\"num_scales must be positive.\")\n",
        "\n",
        "    if num_layers == 0:\n",
        "        print(f'No layers defined (num_layers=0), 0 paths created.')\n",
        "        return tuple(), 0\n",
        "\n",
        "    # num_scales > num_layers implies more pooling stages than layers (after the first conv) to put them after.\n",
        "    if num_scales > num_layers :\n",
        "         raise ValueError(f\"num_scales ({num_scales}) cannot exceed num_layers ({num_layers}) for this search space design when num_layers > 0.\")\n",
        "\n",
        "    num_pooling = num_scales - 1\n",
        "    # num_available_positions_for_pooling refers to the number of positions *after* the initial conv block\n",
        "    # and after each subsequent ResBasicBlock where a pooling operation could be inserted.\n",
        "    # If path has length num_layers, and path[0] is before first block (always 0 pool),\n",
        "    # then there are num_layers-1 subsequent positions for pooling.\n",
        "    num_available_positions_for_pooling = num_layers - 1\n",
        "\n",
        "    if num_pooling < 0 : # Should not happen if num_scales is positive\n",
        "        num_pooling = 0 # Corrects to no pooling if num_scales = 0 somehow (though arg check prevents)\n",
        "\n",
        "    # This condition checks if we're trying to place more pooling layers than available slots\n",
        "    if num_pooling > 0 and num_available_positions_for_pooling < num_pooling :\n",
        "        raise ValueError(\n",
        "            f\"Cannot place {num_pooling} pooling layers in {num_available_positions_for_pooling} available slots \"\n",
        "            f\"(num_layers={num_layers}, num_scales={num_scales}).\"\n",
        "        )\n",
        "\n",
        "    paths = []\n",
        "    # combinations(range(N), k)\n",
        "    # N = num_available_positions_for_pooling\n",
        "    # k = num_pooling\n",
        "    # if N=0 (num_layers=1), k=0 (num_scales=1) => combinations(range(0),0) gives one empty tuple. p=[]. path=[0]. Correct.\n",
        "    # if N=0 (num_layers=1), k>0 (num_scales>1) => combinations(range(0),k) gives zero items. paths list remains empty. Correct.\n",
        "\n",
        "    for positions in combinations(range(num_available_positions_for_pooling), num_pooling):\n",
        "        p = [0] * num_available_positions_for_pooling\n",
        "        for i in positions:\n",
        "            p[i] = 1\n",
        "        paths.append(tuple([0] + p)) # path[0] is always 0 (no pooling before the first block)\n",
        "\n",
        "    paths_tuple = tuple(paths)\n",
        "    number_paths = len(paths_tuple)\n",
        "\n",
        "    # This specific check handles the case where num_layers=1, num_scales>1 leading to 0 paths\n",
        "    if number_paths == 0 and num_layers > 0:\n",
        "        if not (num_available_positions_for_pooling == 0 and num_pooling > 0):\n",
        "             # If it's not the expected C(0, k>0) case, then it's an unexpected empty path list\n",
        "             print(f\"Warning: No paths generated for num_layers={num_layers}, num_scales={num_scales}, \"\n",
        "                   f\"num_pooling={num_pooling}, num_available_positions={num_available_positions_for_pooling}. \"\n",
        "                   f\"This might be an issue if paths were expected.\")\n",
        "\n",
        "    print(f'All {number_paths} paths created.')\n",
        "    if 0 < number_paths < 20: # Print paths only if a small number for brevity\n",
        "        print(paths_tuple)\n",
        "    return paths_tuple, number_paths\n",
        "\n",
        "\n",
        "def init_path_logit(num_paths: int, initial_logits: float = 1.0) -> torch.Tensor:\n",
        "    if num_paths < 0:\n",
        "        raise ValueError(\"Number of paths cannot be negative.\")\n",
        "    if num_paths == 0:\n",
        "        return torch.FloatTensor([])\n",
        "    initial_path_weights = torch.FloatTensor([initial_logits for _ in range(num_paths)])\n",
        "    return initial_path_weights\n",
        "\n",
        "def sample_uniform(sample_weights: torch.Tensor, paths: tuple) -> Tuple[int, tuple]:\n",
        "    if not paths:\n",
        "        raise ValueError(\"Cannot sample from empty paths tuple.\")\n",
        "    if sample_weights.nelement() == 0:\n",
        "        raise ValueError(\"Cannot sample path: sample_weights tensor is empty (likely num_paths was 0).\")\n",
        "\n",
        "    if sample_weights.dim() > 1:\n",
        "        sample_weights = sample_weights.squeeze()\n",
        "        if sample_weights.dim() > 1:\n",
        "            raise ValueError(\"sample_weights must be a 1D tensor of logits.\")\n",
        "    if sample_weights.nelement() != len(paths):\n",
        "        raise ValueError(f\"Number of sample_weights ({sample_weights.nelement()}) must match number of paths ({len(paths)}).\")\n",
        "\n",
        "    try:\n",
        "        prob_distribution = Categorical(logits=sample_weights)\n",
        "    except RuntimeError as e:\n",
        "        if not torch.isfinite(sample_weights).all():\n",
        "            raise ValueError(\"Logits in sample_weights must be finite.\") from e\n",
        "        raise\n",
        "    path_index = int(prob_distribution.sample().item())\n",
        "    if not (0 <= path_index < len(paths)):\n",
        "        raise IndexError(f\"Sampled path_index {path_index} is out of bounds for paths list of length {len(paths)}.\")\n",
        "    return path_index, paths[path_index]\n",
        "\n",
        "\n",
        "# --- From lr_scheduler.py ---\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    def __init__(self,\n",
        "                 optimizer : torch.optim.Optimizer,\n",
        "                 first_cycle_steps : int,\n",
        "                 cycle_mult : float = 1.,\n",
        "                 max_lr : float = 0.1,\n",
        "                 min_lr : float = 0.001,\n",
        "                 warmup_steps : int = 0,\n",
        "                 gamma : float = 1.,\n",
        "                 last_epoch : int = -1\n",
        "        ):\n",
        "        if not first_cycle_steps > 0:\n",
        "            raise ValueError(\"first_cycle_steps must be positive.\")\n",
        "        if warmup_steps < 0:\n",
        "            raise ValueError(\"warmup_steps cannot be negative.\")\n",
        "        if warmup_steps >= first_cycle_steps:\n",
        "            raise ValueError(\"warmup_steps must be less than first_cycle_steps.\")\n",
        "\n",
        "        self.first_cycle_steps = first_cycle_steps\n",
        "        self.cycle_mult = cycle_mult\n",
        "        self.base_max_lr = max_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.min_lr = min_lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.gamma = gamma\n",
        "        self.cur_cycle_steps = first_cycle_steps\n",
        "        self.cycle = 0\n",
        "        self.step_in_cycle = last_epoch\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "        self.init_lr()\n",
        "\n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            if self.warmup_steps == 0: # Avoid division by zero if no warmup\n",
        "                 return [self.max_lr for _ in self.base_lrs]\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            denominator = (self.cur_cycle_steps - self.warmup_steps)\n",
        "            if denominator <= 0: # Should be > 0 if cur_cycle_steps > warmup_steps\n",
        "                 return [self.min_lr for _ in self.base_lrs] # Or max_lr if cycle just ended? min_lr seems safer for cosine end.\n",
        "            return [base_lr + (self.max_lr - base_lr) *\n",
        "                    (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps) / denominator)) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch: Optional[int] = None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    if self.cycle_mult <= 1.0: # Non-increasing cycle length\n",
        "                        print(f\"Warning: CosineAnnealingWarmupRestarts encountered cycle_mult={self.cycle_mult} <= 1.0. Using simpler cycle calculation.\")\n",
        "                        self.cycle = epoch // self.first_cycle_steps if self.first_cycle_steps > 0 else 0\n",
        "                        self.step_in_cycle = epoch % self.first_cycle_steps if self.first_cycle_steps > 0 else epoch\n",
        "                        self.cur_cycle_steps = self.first_cycle_steps\n",
        "                    else: # Increasing cycle length\n",
        "                        # Formula for sum of geometric series to find which cycle 'epoch' falls into\n",
        "                        log_arg = (epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1) if self.first_cycle_steps > 0 else 1.0\n",
        "                        if log_arg <= 0: # Should not happen with cycle_mult > 1 and epoch >= 0\n",
        "                             print(f\"Warning: CosineAnnealingWarmupRestarts encountered non-positive log argument: {log_arg}. Using simpler cycle calculation.\")\n",
        "                             self.cycle = epoch // self.first_cycle_steps if self.first_cycle_steps > 0 else 0\n",
        "                             self.step_in_cycle = epoch % self.first_cycle_steps if self.first_cycle_steps > 0 else epoch\n",
        "                             self.cur_cycle_steps = self.first_cycle_steps\n",
        "                        else:\n",
        "                            n = int(math.log(log_arg, self.cycle_mult))\n",
        "                            self.cycle = n\n",
        "                            # Sum of first n terms of geometric series: T_0 * (mult^n - 1) / (mult - 1)\n",
        "                            denominator_cycle_sum = (self.cycle_mult - 1)\n",
        "                            # if denominator_cycle_sum == 0: # Caught by cycle_mult == 1.\n",
        "                            # sum_geometric_progression = self.first_cycle_steps * (self.cycle + 1)\n",
        "                            # else:\n",
        "                            sum_geometric_progression = self.first_cycle_steps * (self.cycle_mult**n - 1) / denominator_cycle_sum\n",
        "                            self.step_in_cycle = epoch - int(sum_geometric_progression)\n",
        "                            self.cur_cycle_steps = int(self.first_cycle_steps * self.cycle_mult**(n))\n",
        "            else: # Before the first cycle completes\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "\n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr_val in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr_val\n",
        "\n",
        "\n",
        "# --- From create_model.py ---\n",
        "def pooling_func(x: torch.Tensor) -> torch.Tensor:\n",
        "    out = F.max_pool2d(x, kernel_size=2)\n",
        "    return out\n",
        "\n",
        "class ResBasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes: int, out_planes: int, stride: int = 1, downsample: Optional[nn.Module] = None):\n",
        "        super(ResBasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_planes, affine=False)#, track_running_stats=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes, affine=False)#, track_running_stats=False)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class BasicConvBlock(nn.Module):\n",
        "    def __init__(self, in_planes: int, out_planes: int, kernel_size: int = 3):\n",
        "        super(BasicConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_planes, affine=False)#, track_running_stats=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet20(nn.Module): # MODIFIED CLASS\n",
        "    def __init__(self, block_type: Type[ResBasicBlock], num_layers: int, channels: List[int], num_classes: int = 10):\n",
        "        super(ResNet20, self).__init__()\n",
        "        if num_layers <= 0:\n",
        "            raise ValueError(\"num_layers must be positive.\")\n",
        "        if len(channels) != num_layers:\n",
        "            raise ValueError(f\"Length of 'channels' list ({len(channels)}) must match 'num_layers' ({num_layers}).\")\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        layers_list = [BasicConvBlock(3, channels[0])] # Input channels for image is 3\n",
        "        self.current_inplanes = channels[0] # Output of the first block\n",
        "\n",
        "        for i in range(1, num_layers):\n",
        "            layers_list.append(self._make_layer(block_type, channels[i]))\n",
        "            # self.current_inplanes is updated to channels[i] inside _make_layer\n",
        "\n",
        "        self.res_blocks = nn.ModuleList(layers_list)\n",
        "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.current_inplanes, num_classes, bias=False) # Uses output channels of the last block\n",
        "\n",
        "        self.path_pooling_config: Optional[Tuple[int, ...]] = None\n",
        "\n",
        "    def _make_layer(self, block_type: Type[ResBasicBlock], planes: int, stride_for_block: int = 1) -> nn.Module:\n",
        "        downsample_shortcut = None\n",
        "        if self.current_inplanes != planes:\n",
        "            downsample_shortcut = nn.Sequential(\n",
        "                nn.Conv2d(self.current_inplanes, planes, kernel_size=1, stride=1, bias=False),\n",
        "                nn.BatchNorm2d(planes, affine=False)#, track_running_stats=False),\n",
        "            )\n",
        "        layer = block_type(self.current_inplanes, planes, stride=stride_for_block, downsample=downsample_shortcut)\n",
        "        self.current_inplanes = planes # Update for the next layer\n",
        "        return layer\n",
        "\n",
        "    def set_path(self, path: Tuple[int, ...]):\n",
        "        if len(path) != self.num_layers:\n",
        "            raise ValueError(f\"Path length ({len(path)}) must match number of layers ({self.num_layers}).\")\n",
        "        self.path_pooling_config = path\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.path_pooling_config is None:\n",
        "            raise RuntimeError(\"Path not set. Call set_path(path) before forward pass.\")\n",
        "\n",
        "        out = x\n",
        "        for c in range(self.num_layers):\n",
        "            if self.path_pooling_config[c]:\n",
        "                out = pooling_func(out)\n",
        "            out = self.res_blocks[c](out)\n",
        "\n",
        "        out = self.adaptive_avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "    def feature_extractor(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extracts feature maps from the network.\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "        Returns:\n",
        "            List[torch.Tensor]: A list of feature maps.\n",
        "                                - Feature maps after each block in self.res_blocks.\n",
        "                                - Feature map after self.adaptive_avg_pool.\n",
        "        \"\"\"\n",
        "        if self.path_pooling_config is None:\n",
        "            raise RuntimeError(\"Path not set. Call set_path(path) before calling feature_extractor.\")\n",
        "\n",
        "        feature_maps: List[torch.Tensor] = []\n",
        "        out = x\n",
        "        for c in range(self.num_layers):\n",
        "            if self.path_pooling_config[c]:\n",
        "                out = pooling_func(out)\n",
        "            out = self.res_blocks[c](out)\n",
        "            feature_maps.append(out.detach().clone())\n",
        "\n",
        "        out = self.adaptive_avg_pool(out)\n",
        "        feature_maps.append(out.detach().clone())\n",
        "        return feature_maps\n",
        "\n",
        "\n",
        "# --- From data_loader.py ---\n",
        "def _data_transforms_cifar10() -> Tuple[transforms.Compose, transforms.Compose]:\n",
        "    cifar_mean = [0.49139968, 0.48215827, 0.44653124]\n",
        "    cifar_std = [0.24703233, 0.24348505, 0.26158768]\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std),\n",
        "    ])\n",
        "    return train_transform, test_transform\n",
        "\n",
        "def _validation_set_indices(total_train_samples: int, valid_percent: float) -> Tuple[List[int], List[int]]:\n",
        "    if not (0.0 <= valid_percent < 1.0):\n",
        "        raise ValueError(\"valid_percent must be between 0.0 and almost 1.0.\")\n",
        "    num_validation_samples = int(valid_percent * total_train_samples)\n",
        "    num_train_samples = total_train_samples - num_validation_samples\n",
        "    print(f'Total original training samples: {total_train_samples}')\n",
        "    print(f'New training size: {num_train_samples}, Validation size: {num_validation_samples}')\n",
        "    indexes = list(range(total_train_samples))\n",
        "    random.shuffle(indexes)\n",
        "    train_indices = indexes[:num_train_samples]\n",
        "    val_indices = indexes[num_train_samples:]\n",
        "    return train_indices, val_indices\n",
        "\n",
        "def get_data_loaders(\n",
        "    dataset_name: str,\n",
        "    valid_percent: float,\n",
        "    batch_size: int,\n",
        "    dataset_dir: str = '~/data/', # Consider using os.path.expanduser for ~\n",
        "    workers: int = 2\n",
        ") -> Tuple[DataLoader, Optional[DataLoader], Tuple[List[int], List[int]]]:\n",
        "    dataset_dir = os.path.expanduser(dataset_dir) # Added expanduser\n",
        "    if dataset_name == 'CIFAR10':\n",
        "        train_transform_CIFAR, test_transform_CIFAR = _data_transforms_cifar10()\n",
        "        full_trainset = torchvision.datasets.CIFAR10(root=dataset_dir, train=True, download=True, transform=train_transform_CIFAR)\n",
        "        # For validation, use the test transform (no augmentation) but on the training data split\n",
        "        valset_for_loader = torchvision.datasets.CIFAR10(root=dataset_dir, train=True, download=True, transform=test_transform_CIFAR)\n",
        "    else:\n",
        "        raise Exception(f'Dataset {dataset_name} not supported!')\n",
        "\n",
        "    total_train_samples = len(full_trainset)\n",
        "    train_indices, val_indices = _validation_set_indices(total_train_samples, valid_percent)\n",
        "    data_split_indices = (train_indices, val_indices)\n",
        "\n",
        "    train_loader = DataLoader(full_trainset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices), num_workers=workers, pin_memory=True, drop_last=True)\n",
        "    validation_loader = None\n",
        "    if valid_percent > 0.0 and val_indices: # Ensure val_indices is not empty\n",
        "        validation_loader = DataLoader(valset_for_loader, batch_size=batch_size, sampler=SubsetRandomSampler(val_indices), num_workers=workers, pin_memory=True, drop_last=False)\n",
        "    return train_loader, validation_loader, data_split_indices\n",
        "\n",
        "\n",
        "# --- From NAS_trainer.py (now ModelTrainer) ---\n",
        "def create_single_model(layers: int, channels: List[int], num_classes: int = 10) -> nn.Module: # Added num_classes\n",
        "    model = ResNet20(ResBasicBlock, layers, channels, num_classes=num_classes) # Pass num_classes\n",
        "    return model\n",
        "\n",
        "def create_optimizer_and_scheduler(\n",
        "    sched_type: str,\n",
        "    model: nn.Module,\n",
        "    lr: float,\n",
        "    momentum: float,\n",
        "    weight_decay: float,\n",
        "    epochs: int, # Total epochs for CosineAnnealingLR\n",
        "    min_lr: float, # For schedulers\n",
        "    first_cycle_steps: int, # For CosineAnnealingWarmupRestarts\n",
        "    cycle_mult: float, # For CosineAnnealingWarmupRestarts\n",
        "    warmup_steps: int, # For CosineAnnealingWarmupRestarts\n",
        "    gamma: float # For CosineAnnealingWarmupRestarts\n",
        ") -> Tuple[optim.Optimizer, Optional[_LRScheduler]]:\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = None\n",
        "    if sched_type == 'cosine_anneal':\n",
        "        # epochs + 1 might be if T_max is total iterations, or if epochs is 0-indexed.\n",
        "        # Usually T_max is the total number of epochs.\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=min_lr)\n",
        "    elif sched_type == 'cosine_anneal_wr':\n",
        "        scheduler = CosineAnnealingWarmupRestarts(optimizer, first_cycle_steps, cycle_mult, lr, min_lr, warmup_steps, gamma)\n",
        "    elif sched_type == 'none' or sched_type is None:\n",
        "        print(\"No learning rate scheduler will be used.\")\n",
        "    else:\n",
        "        print(f\"Warning: Unsupported scheduler type: '{sched_type}'. No scheduler will be used.\")\n",
        "    return optimizer, scheduler\n",
        "\n",
        "def train_epoch_random_paths(\n",
        "    model: nn.Module, # Should be ResNet20 instance\n",
        "    train_queue: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    criterion: nn.Module,\n",
        "    all_paths: tuple, # Tuple of path tuples\n",
        "    uniform_path_weights: torch.Tensor, # Logits for sampling paths\n",
        "    device: torch.device\n",
        ") -> Tuple[float, float]: # Returns (average_loss, accuracy)\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    if len(train_queue) == 0:\n",
        "        print(\"Warning: train_queue is empty. Skipping training for this epoch.\")\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    # Move weights to device once if they aren't already\n",
        "    if uniform_path_weights.device != device:\n",
        "        uniform_path_weights = uniform_path_weights.to(device)\n",
        "\n",
        "    for inputs, targets in train_queue:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        _, current_path = sample_uniform(uniform_path_weights, all_paths) # sample_uniform expects weights, paths\n",
        "\n",
        "        if not hasattr(model, 'set_path'):\n",
        "            raise TypeError(\"Model does not have a 'set_path' method. Ensure it's the ResNet20 class.\")\n",
        "        model.set_path(current_path) # Type assertion for model if needed\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted_classes = outputs.max(1)\n",
        "        total_samples += targets.size(0)\n",
        "        correct_predictions += predicted_classes.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_queue) if len(train_queue) > 0 else 0.0\n",
        "    accuracy = (100. * correct_predictions / total_samples) if total_samples > 0 else 0.0\n",
        "    # Consider printing epoch number if available\n",
        "    print(f'Training Epoch: Avg Loss: {avg_loss:.4f}, Accuracy: {correct_predictions}/{total_samples} ({accuracy:.2f}%)')\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "YNzVJ-nR83UF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractor Functions\n",
        "Output, feature_mpars, Gradient extractor"
      ],
      "metadata": {
        "id": "C3ggOrU8_L0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output_extractor(model: nn.Module, val_inputs: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
        "    model.train()\n",
        "    val_inputs = val_inputs.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(val_inputs)\n",
        "    return outputs.detach().cpu()\n",
        "\n",
        "def feature_maps_extractor(model: nn.Module, val_inputs: torch.Tensor, device: torch.device) -> List[torch.Tensor]:\n",
        "    model.train()\n",
        "    val_inputs = val_inputs.to(device)\n",
        "    with torch.no_grad():\n",
        "        feature_maps = model.feature_extractor(val_inputs)\n",
        "    return feature_maps\n",
        "\n",
        "def gradients_extractor(model: nn.Module, val_inputs: torch.Tensor, val_targets: torch.Tensor, device: torch.device) -> Dict[str, Optional[torch.Tensor]]:\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    val_inputs = val_inputs.to(device).detach().clone()\n",
        "    val_targets = val_targets.to(device).detach().clone() # Corrected to use val_targets\n",
        "    outputs = model(val_inputs)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    loss = criterion(outputs, val_targets)\n",
        "    loss.backward()\n",
        "    current_path_gradients: Dict[str, Optional[torch.Tensor]] = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            current_path_gradients[name] = param.grad.clone().detach().cpu()\n",
        "        else:\n",
        "            # This case can happen if a parameter was not used in the forward pass for this specific path\n",
        "            current_path_gradients[name] = None\n",
        "    return current_path_gradients"
      ],
      "metadata": {
        "id": "DuCkk2N2_Upx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction"
      ],
      "metadata": {
        "id": "HC4TMO-tEq2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model_all_paths_on_batch(\n",
        "    model: nn.Module, # Should be ResNet20 instance\n",
        "    all_paths: tuple, # Tuple of path tuples\n",
        "    val_inputs: torch.Tensor,\n",
        "    val_targets: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device\n",
        ") -> Tuple[Dict[str, torch.Tensor], Dict[str, List[torch.Tensor]], Dict[str, Dict[str, Optional[torch.Tensor]]]]: # Updated return type hint\n",
        "    # model.eval()\n",
        "    #### We need to haave batchnorm in training mode\n",
        "    model.train()\n",
        "    output_vectors_all_paths: Dict[str, torch.Tensor] = {}\n",
        "    feature_maps_all_paths: Dict[str, List[torch.Tensor]] = {} # Initialized feature_maps_all_paths\n",
        "    gradients_all_paths: Dict[str, Dict[str, Optional[torch.Tensor]]] = {}\n",
        "\n",
        "    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
        "\n",
        "    print(f\"Starting validation for {len(all_paths)} paths on a single mini-batch...\")\n",
        "    if not all_paths:\n",
        "        print(\"Warning: No paths provided for validation.\")\n",
        "        return output_vectors_all_paths, feature_maps_all_paths, gradients_all_paths # Include feature_maps_all_paths in empty return\n",
        "\n",
        "    if not hasattr(model, 'set_path'):\n",
        "        raise TypeError(\"Model does not have a 'set_path' method. Ensure it's the ResNet20 class.\")\n",
        "\n",
        "    for i, path_tuple in enumerate(all_paths):\n",
        "        path_key = str(path_tuple)\n",
        "        # print(f\"  Validating path {i+1}/{len(all_paths)}: {path_tuple}\") # Can be very verbose\n",
        "\n",
        "        # Ensure model is zero_grad for each path's gradient calculation\n",
        "        model.zero_grad()\n",
        "        model.set_path(path_tuple)\n",
        "\n",
        "        # Get output logits without gradients for storage\n",
        "        with torch.no_grad():\n",
        "            outputs_no_grad = output_extractor(model, val_inputs, device)\n",
        "\n",
        "        # Get feature maps without gradients for storage\n",
        "        # with torch.no_grad():\n",
        "        #     feature_maps = feature_maps_extractor(model, val_inputs, device)\n",
        "        output_vectors_all_paths[path_key] = outputs_no_grad\n",
        "        # feature_maps_all_paths[path_key] = feature_maps\n",
        "\n",
        "        ### Gradients\n",
        "\n",
        "        # Re-enable grad for loss calculation and backward pass for this specific path\n",
        "        # Detach inputs if they came from a previous computation graph part not relevant here\n",
        "        # outputs_for_loss = model(val_inputs.detach().clone()) # Use detached clone for clean grad calculation\n",
        "        # loss = criterion(outputs_for_loss, val_targets)\n",
        "        # loss.backward() # Accumulates gradients in model.parameters() for the current path\n",
        "\n",
        "        # current_path_gradients: Dict[str, Optional[torch.Tensor]] = {}\n",
        "        # for name, param in model.named_parameters():\n",
        "        #     if param.grad is not None:\n",
        "        #         current_path_gradients[name] = param.grad.clone().detach().cpu()\n",
        "        #     else:\n",
        "        #         # This case can happen if a parameter was not used in the forward pass for this specific path\n",
        "        #         current_path_gradients[name] = None\n",
        "\n",
        "        # Corrected argument order for gradients_extractor\n",
        "        # gradients_all_paths[path_key] = gradients_extractor(model, val_inputs, val_targets, device)\n",
        "\n",
        "        # It's crucial to zero_grad() again if optimizer.step() is not called,\n",
        "        # or before the next path's backward(). model.zero_grad() at start of loop handles this.\n",
        "\n",
        "    print(f\"Finished validation for all {len(all_paths)} paths.\")\n",
        "    return output_vectors_all_paths, feature_maps_all_paths, gradients_all_paths # Include feature_maps_all_paths in return"
      ],
      "metadata": {
        "id": "PLeuuDul-Npq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load saved file\n",
        "At a given epoch for a given run"
      ],
      "metadata": {
        "id": "m8sFS0pz8DW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F7e2Mk9Q7-7M"
      },
      "outputs": [],
      "source": [
        "test_number = 3001\n",
        "analysis_epoch = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOad and save output etc"
      ],
      "metadata": {
        "id": "RgUybrb60k1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "# analysis_epochs = [1, 6, 11, 16, 26, 46, 71, 101, 126, 151, 156, 171, 176, 196]\n",
        "analysis_epochs = [1]"
      ],
      "metadata": {
        "id": "gsmF3dyT0kYX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for analysis_epoch in analysis_epochs:\n",
        "  print(\"epoch:\", analysis_epoch)\n",
        "  print('------------------------')\n",
        "  gc.collect()\n",
        "  load_save_for_epoch(test_number, analysis_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VESOj4LR1Cs8",
        "outputId": "20edad21-3a1f-4685-9018-075d949958e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "------------------------\n",
            "Successfully loaded checkpoint from /content/drive/MyDrive/paper4/pooling/test_3001/model_checkpoint_epoch_1_begin.pt\n",
            "All 36 paths created.\n",
            "Starting validation for 36 paths on a single mini-batch...\n",
            "Finished validation for all 36 paths.\n",
            "Outputs saved to /content/drive/MyDrive/paper4/pooling/test_3001/outputs/outputs_epoch_1.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_save_for_epoch(test_number, analysis_epoch):\n",
        "  '''Load data'''\n",
        "  # prompt: torch.load from the  /content/drive/MyDrive/paper4/pooling/test_\"test_number\"/model_checkpoint_epoch_\"analysis_epoch\"_begin.pt\n",
        "  # Ensure the path exists and is correct\n",
        "  try:\n",
        "    del checkpoint, model, output_vectors_all_paths, feature_maps_all_paths, gradients_all_paths\n",
        "  except: pass\n",
        "\n",
        "  gc.collect()\n",
        "  checkpoint_path = f\"/content/drive/MyDrive/paper4/pooling/test_{test_number}/model_checkpoint_epoch_{analysis_epoch}_begin.pt\"\n",
        "\n",
        "  # Load the model checkpoint\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  print(f\"Successfully loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "  channels = string_to_list(\"16,16,16,16,32,32,32,64,64,64\", 10)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = create_single_model(10, channels, num_classes=10).to(device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  '''Validate'''\n",
        "  gc.collect()\n",
        "  # get val minibatch\n",
        "  val_data = \"/content/drive/MyDrive/paper4/pooling/test_\" + str(test_number) + \"/extracted_val_batch.pt\"\n",
        "  saved_batch_data = torch.load(val_data, map_location=device)\n",
        "  inputs = saved_batch_data['inputs']\n",
        "  targets = saved_batch_data['targets']\n",
        "  all_paths, num_paths = create_search_space(10, 3)\n",
        "  criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  output_vectors_all_paths, feature_maps_all_paths, gradients_all_paths = validate_model_all_paths_on_batch(model, all_paths, inputs, targets, criterion, device)\n",
        "\n",
        "  '''Save'''\n",
        "  # Define the base directory for saving\n",
        "  base_save_dir = f\"/content/drive/MyDrive/paper4/pooling/test_{test_number}/\"\n",
        "\n",
        "  # Create the main test directory if it doesn't exist\n",
        "  os.makedirs(base_save_dir, exist_ok=True)\n",
        "\n",
        "  # Define subdirectories\n",
        "  outputs_dir = os.path.join(base_save_dir, \"outputs\")\n",
        "  feature_maps_dir = os.path.join(base_save_dir, \"feature_maps\")\n",
        "  gradients_dir = os.path.join(base_save_dir, \"gradients\")\n",
        "\n",
        "  # Create subdirectories if they don't exist\n",
        "  os.makedirs(outputs_dir, exist_ok=True)\n",
        "  os.makedirs(feature_maps_dir, exist_ok=True)\n",
        "  os.makedirs(gradients_dir, exist_ok=True)\n",
        "\n",
        "  # Save outputs\n",
        "  output_save_path = os.path.join(outputs_dir, f\"outputs_epoch_{analysis_epoch}.pt\")\n",
        "  torch.save(output_vectors_all_paths, output_save_path)\n",
        "  print(f\"Outputs saved to {output_save_path}\")\n",
        "\n",
        "  # # Save feature maps\n",
        "  # feature_maps_save_path = os.path.join(feature_maps_dir, f\"feature_maps_epoch_{analysis_epoch}.pt\")\n",
        "  # torch.save(feature_maps_all_paths, feature_maps_save_path)\n",
        "  # print(f\"Feature maps saved to {feature_maps_save_path}\")\n",
        "\n",
        "  # Save gradients\n",
        "  # gradients_save_path = os.path.join(gradients_dir, f\"gradients_epoch_{analysis_epoch}.pt\")\n",
        "  # torch.save(gradients_all_paths, gradients_save_path)\n",
        "  # print(f\"Gradients saved to {gradients_save_path}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9a478Faj1OVj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "GH20dRzaHRM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# prompt: torch.load from the  /content/drive/MyDrive/paper4/pooling/test_\"test_number\"/model_checkpoint_epoch_\"analysis_epoch\"_begin.pt\n",
        "# Ensure the path exists and is correct\n",
        "try:\n",
        "  del checkpoint, model, output_vectors_all_paths, feature_maps_all_paths, gradients_all_paths\n",
        "except: pass\n",
        "\n",
        "gc.collect()\n",
        "checkpoint_path = f\"/content/drive/MyDrive/paper4/pooling/test_{test_number}/model_checkpoint_epoch_{analysis_epoch}_begin.pt\"\n",
        "\n",
        "# Load the model checkpoint\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "print(f\"Successfully loaded checkpoint from {checkpoint_path}\")\n",
        "\n",
        "channels = string_to_list(\"16,16,16,16,32,32,32,64,64,64\", 10)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = create_single_model(10, channels, num_classes=10).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtgREtauFJl9",
        "outputId": "be85b9ed-7a83-48fb-9233-fc289ba60809"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded checkpoint from /content/drive/MyDrive/paper4/pooling/test_3001/model_checkpoint_epoch_1_begin.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate and save"
      ],
      "metadata": {
        "id": "ylPLt266HZ3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "# get val minibatch\n",
        "saved_batch_data = torch.load(\"/content/drive/MyDrive/paper4/pooling/test_3000/extracted_val_batch.pt\", map_location=device)\n",
        "inputs = saved_batch_data['inputs']\n",
        "targets = saved_batch_data['targets']\n",
        "all_paths, num_paths = create_search_space(10, 3)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "output_vectors_all_paths, feature_maps_all_paths, gradients_all_paths = validate_model_all_paths_on_batch(model, all_paths, inputs, targets, criterion, device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pYXNO5S0Hco6",
        "outputId": "a38e5d69-f640-407e-b4cb-b83b3ce5ddb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 36 paths created.\n",
            "Starting validation for 36 paths on a single mini-batch...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 179.38 MiB is free. Process 233044 has 21.98 GiB memory in use. Of the allocated memory 21.18 GiB is allocated by PyTorch, and 578.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-4206341289.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0moutput_vectors_all_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps_all_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients_all_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model_all_paths_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-3585639142.py\u001b[0m in \u001b[0;36mvalidate_model_all_paths_on_batch\u001b[0;34m(model, all_paths, val_inputs, val_targets, criterion, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Corrected argument order for gradients_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mgradients_all_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# It's crucial to zero_grad() again if optimizer.step() is not called,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-3296279803.py\u001b[0m in \u001b[0;36mgradients_extractor\u001b[0;34m(model, val_inputs, val_targets, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mval_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Corrected to use val_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-8581328.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_pooling_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooling_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-8581328.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 179.38 MiB is free. Process 233044 has 21.98 GiB memory in use. Of the allocated memory 21.18 GiB is allocated by PyTorch, and 578.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save outputs, feature maps and gradietns in separate folders in the directory   /content/drive/MyDrive/paper4/pooling/test_\"test_number\"/\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the base directory for saving\n",
        "base_save_dir = f\"/content/drive/MyDrive/paper4/pooling/test_{test_number}/\"\n",
        "\n",
        "# Create the main test directory if it doesn't exist\n",
        "os.makedirs(base_save_dir, exist_ok=True)\n",
        "\n",
        "# Define subdirectories\n",
        "outputs_dir = os.path.join(base_save_dir, \"outputs\")\n",
        "feature_maps_dir = os.path.join(base_save_dir, \"feature_maps\")\n",
        "gradients_dir = os.path.join(base_save_dir, \"gradients\")\n",
        "\n",
        "# Create subdirectories if they don't exist\n",
        "os.makedirs(outputs_dir, exist_ok=True)\n",
        "os.makedirs(feature_maps_dir, exist_ok=True)\n",
        "os.makedirs(gradients_dir, exist_ok=True)\n",
        "\n",
        "# Save outputs\n",
        "output_save_path = os.path.join(outputs_dir, f\"outputs_epoch_{analysis_epoch}.pt\")\n",
        "torch.save(output_vectors_all_paths, output_save_path)\n",
        "print(f\"Outputs saved to {output_save_path}\")\n",
        "\n",
        "# Save feature maps\n",
        "feature_maps_save_path = os.path.join(feature_maps_dir, f\"feature_maps_epoch_{analysis_epoch}.pt\")\n",
        "torch.save(feature_maps_all_paths, feature_maps_save_path)\n",
        "print(f\"Feature maps saved to {feature_maps_save_path}\")\n",
        "\n",
        "# Save gradients\n",
        "# gradients_save_path = os.path.join(gradients_dir, f\"gradients_epoch_{analysis_epoch}.pt\")\n",
        "# torch.save(gradients_all_paths, gradients_save_path)\n",
        "# print(f\"Gradients saved to {gradients_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY2BR93VLDG_",
        "outputId": "3ead267a-3efd-445c-91fb-460f0744e26f"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature maps saved to /content/drive/MyDrive/paper4/pooling/test_3002/feature_maps/feature_maps_epoch_196.pt\n",
            "Gradients saved to /content/drive/MyDrive/paper4/pooling/test_3002/gradients/gradients_epoch_196.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ♒ Extract Val Batch"
      ],
      "metadata": {
        "id": "qCVKICj0LJO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_split_indices = checkpoint.get('data_split_indices')\n",
        "_, val_indices = data_split_indices\n",
        "\n",
        "output_batch_filename = f\"extracted_val_batch.pt\"\n",
        "output_batch_filepath = os.path.join(os.path.dirname(f\"/content/drive/MyDrive/paper4/pooling/test_{test_number}/\"), output_batch_filename)\n",
        "\n",
        "def _data_transforms_cifar10_validation_only() -> transforms.Compose:\n",
        "    cifar_mean = [0.49139968, 0.48215827, 0.44653124]\n",
        "    cifar_std = [0.24703233, 0.24348505, 0.26158768]\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std),\n",
        "    ])\n",
        "    return test_transform\n",
        "\n",
        "if 1:\n",
        "    val_transform = _data_transforms_cifar10_validation_only()\n",
        "    full_original_train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./', train=True, download=True, transform=val_transform\n",
        "    )\n",
        "\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "validation_batch_loader = DataLoader(\n",
        "    full_original_train_dataset, batch_size=2000, sampler=val_sampler,\n",
        "    num_workers=4, pin_memory=False\n",
        ")\n",
        "\n",
        "inputs, targets = next(iter(validation_batch_loader))\n",
        "\n",
        "torch.save({'inputs': inputs, 'targets': targets}, output_batch_filepath)\n",
        "print(f\"Saved validation batch: {output_batch_filepath}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URzfD_XYLMAq",
        "outputId": "b7a4a973-7049-4b99-be22-40e4c5c2adf6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved validation batch: /content/drive/MyDrive/paper4/pooling/test_3002/extracted_val_batch.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspection"
      ],
      "metadata": {
        "id": "wK60rQXoNb6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: inspect output_vectors_all_paths, feature_maps_all_paths, gradients_all_paths\n",
        "\n",
        "print(\"\\n--- Inspecting output_vectors_all_paths ---\")\n",
        "print(f\"Type: {type(output_vectors_all_paths)}\")\n",
        "print(f\"Number of paths in dictionary: {len(output_vectors_all_paths)}\")\n",
        "if output_vectors_all_paths:\n",
        "    # Get the first key (path tuple string)\n",
        "    first_path_key = list(output_vectors_all_paths.keys())[0]\n",
        "    print(f\"Example key (path): {first_path_key}\")\n",
        "    first_output_tensor = output_vectors_all_paths[first_path_key]\n",
        "    print(f\"Value type (for path '{first_path_key}'): {type(first_output_tensor)}\")\n",
        "    if isinstance(first_output_tensor, torch.Tensor):\n",
        "        print(f\"Tensor shape: {first_output_tensor.shape}\")\n",
        "        print(f\"Tensor device: {first_output_tensor.device}\")\n",
        "        print(f\"Sample tensor content (first 5 values): {first_output_tensor.flatten()[:5].tolist()}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Inspecting feature_maps_all_paths ---\")\n",
        "print(f\"Type: {type(feature_maps_all_paths)}\")\n",
        "print(f\"Number of paths in dictionary: {len(feature_maps_all_paths)}\")\n",
        "if feature_maps_all_paths:\n",
        "    # Get the first key (path tuple string)\n",
        "    first_path_key = list(feature_maps_all_paths.keys())[0]\n",
        "    print(f\"Example key (path): {first_path_key}\")\n",
        "    first_feature_list = feature_maps_all_paths[first_path_key]\n",
        "    print(f\"Value type (for path '{first_path_key}'): {type(first_feature_list)}\")\n",
        "    if isinstance(first_feature_list, list):\n",
        "        print(f\"Number of feature maps in the list: {len(first_feature_list)}\")\n",
        "        if first_feature_list:\n",
        "            first_feature_tensor = first_feature_list[0]\n",
        "            print(f\"Type of first feature map: {type(first_feature_tensor)}\")\n",
        "            if isinstance(first_feature_tensor, torch.Tensor):\n",
        "                print(f\"Shape of first feature map: {first_feature_tensor.shape}\")\n",
        "                print(f\"Device of first feature map: {first_feature_tensor.device}\")\n",
        "                print(f\"Sample tensor content (first 5 values) of first feature map: {first_feature_tensor.flatten()[:5].tolist()}\")\n",
        "            last_feature_tensor = first_feature_list[-1] # Adaptive AvgPool output\n",
        "            print(f\"Type of last feature map: {type(last_feature_tensor)}\")\n",
        "            if isinstance(last_feature_tensor, torch.Tensor):\n",
        "                 print(f\"Shape of last feature map: {last_feature_tensor.shape}\")\n",
        "                 print(f\"Device of last feature map: {last_feature_tensor.device}\")\n",
        "                 print(f\"Sample tensor content (first 5 values) of last feature map: {last_feature_tensor.flatten()[:5].tolist()}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Inspecting gradients_all_paths ---\")\n",
        "print(f\"Type: {type(gradients_all_paths)}\")\n",
        "print(f\"Number of paths in dictionary: {len(gradients_all_paths)}\")\n",
        "if gradients_all_paths:\n",
        "    # Get the first key (path tuple string)\n",
        "    first_path_key = list(gradients_all_paths.keys())[0]\n",
        "    print(f\"Example key (path): {first_path_key}\")\n",
        "    first_gradients_dict = gradients_all_paths[first_path_key]\n",
        "    print(f\"Value type (for path '{first_path_key}'): {type(first_gradients_dict)}\")\n",
        "    if isinstance(first_gradients_dict, dict):\n",
        "        print(f\"Number of gradient tensors/None in the dictionary: {len(first_gradients_dict)}\")\n",
        "        # Iterate through some items (e.g., first 5)\n",
        "        print(\"Sample gradient entries:\")\n",
        "        for i, (grad_name, grad_tensor) in enumerate(first_gradients_dict.items()):\n",
        "            if i >= 5: break\n",
        "            print(f\"  Parameter name: '{grad_name}'\")\n",
        "            print(f\"    Gradient type: {type(grad_tensor)}\")\n",
        "            if isinstance(grad_tensor, torch.Tensor):\n",
        "                print(f\"    Gradient shape: {grad_tensor.shape}\")\n",
        "                print(f\"    Gradient device: {grad_tensor.device}\") # Should be cpu based on implementation\n",
        "                # print(f\"    Sample tensor content (first 5 values): {grad_tensor.flatten()[:5].tolist()}\") # Might be large\n",
        "            elif grad_tensor is None:\n",
        "                 print(\"    Gradient is None (parameter not used in this path)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD__fj9aNbgg",
        "outputId": "f84d0294-91d8-4e5f-f2e7-ce26011982ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inspecting output_vectors_all_paths ---\n",
            "Type: <class 'dict'>\n",
            "Number of paths in dictionary: 36\n",
            "Example key (path): (0, 1, 1, 0, 0, 0, 0, 0, 0, 0)\n",
            "Value type (for path '(0, 1, 1, 0, 0, 0, 0, 0, 0, 0)'): <class 'builtin_function_or_method'>\n",
            "\n",
            "--- Inspecting feature_maps_all_paths ---\n",
            "Type: <class 'dict'>\n",
            "Number of paths in dictionary: 36\n",
            "Example key (path): (0, 1, 1, 0, 0, 0, 0, 0, 0, 0)\n",
            "Value type (for path '(0, 1, 1, 0, 0, 0, 0, 0, 0, 0)'): <class 'list'>\n",
            "Number of feature maps in the list: 11\n",
            "Type of first feature map: <class 'torch.Tensor'>\n",
            "Shape of first feature map: torch.Size([512, 16, 32, 32])\n",
            "Device of first feature map: cuda:0\n",
            "Sample tensor content (first 5 values) of first feature map: [0.5782656073570251, 0.17400991916656494, 0.0, 0.0, 0.0]\n",
            "Type of last feature map: <class 'torch.Tensor'>\n",
            "Shape of last feature map: torch.Size([512, 64, 1, 1])\n",
            "Device of last feature map: cuda:0\n",
            "Sample tensor content (first 5 values) of last feature map: [0.8793745040893555, 1.169083833694458, 0.8823517560958862, 0.924717903137207, 1.0437082052230835]\n",
            "\n",
            "--- Inspecting gradients_all_paths ---\n",
            "Type: <class 'dict'>\n",
            "Number of paths in dictionary: 36\n",
            "Example key (path): (0, 1, 1, 0, 0, 0, 0, 0, 0, 0)\n",
            "Value type (for path '(0, 1, 1, 0, 0, 0, 0, 0, 0, 0)'): <class 'dict'>\n",
            "Number of gradient tensors/None in the dictionary: 22\n",
            "Sample gradient entries:\n",
            "  Parameter name: 'res_blocks.0.conv.weight'\n",
            "    Gradient type: <class 'torch.Tensor'>\n",
            "    Gradient shape: torch.Size([16, 3, 3, 3])\n",
            "    Gradient device: cpu\n",
            "  Parameter name: 'res_blocks.1.conv1.weight'\n",
            "    Gradient type: <class 'torch.Tensor'>\n",
            "    Gradient shape: torch.Size([16, 16, 3, 3])\n",
            "    Gradient device: cpu\n",
            "  Parameter name: 'res_blocks.1.conv2.weight'\n",
            "    Gradient type: <class 'torch.Tensor'>\n",
            "    Gradient shape: torch.Size([16, 16, 3, 3])\n",
            "    Gradient device: cpu\n",
            "  Parameter name: 'res_blocks.2.conv1.weight'\n",
            "    Gradient type: <class 'torch.Tensor'>\n",
            "    Gradient shape: torch.Size([16, 16, 3, 3])\n",
            "    Gradient device: cpu\n",
            "  Parameter name: 'res_blocks.2.conv2.weight'\n",
            "    Gradient type: <class 'torch.Tensor'>\n",
            "    Gradient shape: torch.Size([16, 16, 3, 3])\n",
            "    Gradient device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h4igZ3P4HSfs"
      }
    }
  ]
}