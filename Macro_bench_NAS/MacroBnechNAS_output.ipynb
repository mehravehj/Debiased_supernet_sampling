{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0y+oy9W0XmLBnAyDkamXb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehravehj/Debiased_supernet_sampling/blob/main/Macro_bench_NAS/MacroBnechNAS_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_loader"
      ],
      "metadata": {
        "id": "ghFkGrfFfQw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXV8_OhSfDnx"
      },
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "def _data_transforms_cifar10():\n",
        "    '''\n",
        "    CIFAR10 data augmentation and normalization\n",
        "    :return: training set transforms, test set transforms\n",
        "    '''\n",
        "    cifar_mean = [0.49139968, 0.48215827, 0.44653124]\n",
        "    cifar_std = [0.24703233, 0.24348505, 0.26158768]\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar_mean, cifar_std),\n",
        "    ])\n",
        "    return train_transform, test_transform\n",
        "def validation_set_indices(num_train, valid_percent):\n",
        "    '''\n",
        "    separate randomly the training set to training and validation sets\n",
        "    :param num_train: training size (currently not used)\n",
        "    :param valid_percent: what portion of training set to be used for validation\n",
        "    :return: a list containing [training set indices, validation set indices]\n",
        "    '''\n",
        "    train_size = num_train - int(valid_percent * num_train)  # number of training examples\n",
        "    val_size = num_train - train_size  # number of validation examples\n",
        "    print('training size:', train_size, ', validation size:', val_size)\n",
        "    indexes = list(range(num_train))  # available indices at training set\n",
        "    shuffle(indexes) # shuffle\n",
        "    indexes = indexes[:num_train] # select the first part\n",
        "    split = train_size\n",
        "    train_index = indexes[:split]\n",
        "    val_index = indexes[split:]\n",
        "    indices = [train_index, val_index]\n",
        "    return indices\n",
        "\n",
        "\n",
        "def data_loader(valid_percent, batch_size, num_train=0, indices=0, dataset_dir='~/Desktop/codes/multires/data/', workers=2):\n",
        "    '''\n",
        "    Load dataset with augmentation and spliting of training and validation set\n",
        "    :param dataset_name: Only for CIFAR10\n",
        "    :param valid_percent: what portion of training set to be used for validation\n",
        "    :param batch_size: batch_size\n",
        "    :param indices: use particular indices rather than randomly separate training set\n",
        "    :param dataset_dir: dataset directory\n",
        "    :param workers: number of workers\n",
        "    :return: train, validation, test data loader, indices, number of classes\n",
        "    '''\n",
        "    train_transform_CIFAR, test_transform_CIFAR = _data_transforms_cifar10()\n",
        "    trainset = torchvision.datasets.CIFAR10(root=dataset_dir, train=True, download=True, transform=train_transform_CIFAR)\n",
        "    valset = torchvision.datasets.CIFAR10(root=dataset_dir, train=True, download=True, transform=test_transform_CIFAR) # no augmentation for validation set\n",
        "    testset = torchvision.datasets.CIFAR10(root=dataset_dir, train=False, download=True, transform=test_transform_CIFAR)\n",
        "    num_class = 10\n",
        "\n",
        "\n",
        "    if not num_train:\n",
        "        num_train = len(trainset)\n",
        "\n",
        "    if not indices: # split and create indices for training and validation set\n",
        "        indices = validation_set_indices(num_train, valid_percent)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=SubsetRandomSampler(indices[0]), num_workers=workers, pin_memory=True, drop_last=True) # load training set\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True, drop_last=True) # load test set\n",
        "    if valid_percent: # load validation set if used\n",
        "        validation_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, sampler=SubsetRandomSampler(indices[1]), num_workers=workers, pin_memory=True, drop_last=True)\n",
        "    else:\n",
        "        validation_loader = 0\n",
        "\n",
        "    return train_loader, validation_loader, test_loader, indices, num_class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "search_space_design"
      ],
      "metadata": {
        "id": "4EyUF0vHfXt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "import torch\n",
        "from torch.distributions.categorical import Categorical as Categorical\n",
        "\n",
        "\n",
        "def create_search_space(num_layers=10, num_scales=3):\n",
        "    # create initial tuple based on layers and scales\n",
        "    num_pooling = num_scales - 1 # number of pooling layers to insert\n",
        "    num_available_layers = num_layers - 1 # number of availble layers to insert pooling on\n",
        "    paths =[]\n",
        "    for positions in combinations(range(num_available_layers), num_pooling):\n",
        "        p = [0] * num_available_layers\n",
        "\n",
        "        for i in positions:\n",
        "            p[i] = 1\n",
        "\n",
        "        # yield tuple(p)\n",
        "\n",
        "        paths.append(tuple([0] + p))\n",
        "    paths = tuple(paths)\n",
        "    number_paths = len(paths)\n",
        "    print('all %d paths created: ' %(number_paths))\n",
        "    print(paths)\n",
        "    return paths, number_paths\n",
        "\n",
        "\n",
        "def sample_uniform(paths,num_paths):\n",
        "    sample_weights = torch.FloatTensor([1 for i in range(num_paths)])  # initialize logits\n",
        "    prob = Categorical(logits=sample_weights)\n",
        "    # print('probabilities')\n",
        "    # print(prob.probs)\n",
        "    path_index = int(prob.sample().data)\n",
        "    # sampled_path = paths[path_index]\n",
        "    return path_index, paths[path_index]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j0GU62vRfXIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utility_functions"
      ],
      "metadata": {
        "id": "YV1_Uf99fgvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_list(x, leng):\n",
        "    if ',' in x:\n",
        "        x = x.split(',')\n",
        "        res = [int(i) for i in x]\n",
        "    else:\n",
        "        res = [int(x) for i in range(leng-1)]\n",
        "\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "dcqlFMGwfk6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lr_scheduler"
      ],
      "metadata": {
        "id": "oWs7shN2f2ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    \"\"\"\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        first_cycle_steps (int): First cycle step size.\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer : torch.optim.Optimizer,\n",
        "                 first_cycle_steps : int,\n",
        "                 cycle_mult : float = 1.,\n",
        "                 max_lr : float = 0.1,\n",
        "                 min_lr : float = 0.001,\n",
        "                 warmup_steps : int = 0,\n",
        "                 gamma : float = 1.,\n",
        "                 last_epoch : int = -1\n",
        "        ):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "\n",
        "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
        "        self.base_max_lr = max_lr # first max learning rate\n",
        "        self.max_lr = max_lr # max learning rate in the current cycle\n",
        "        self.min_lr = min_lr # min learning rate\n",
        "        self.warmup_steps = warmup_steps # warmup step size\n",
        "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
        "\n",
        "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
        "        self.cycle = 0 # cycle count\n",
        "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
        "\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "        # set learning rate min_lr\n",
        "        self.init_lr()\n",
        "\n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "\n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n"
      ],
      "metadata": {
        "id": "Ev4sio0tf455"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAS_trainer"
      ],
      "metadata": {
        "id": "xtbjSMfuftW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "# from utils import *\n",
        "# from utils.lr_scheduler import CosineAnnealingWarmupRestarts\n",
        "# from utils.search_space_design import sample_uniform\n",
        "\n",
        "def create_optimizer(type, net, lr, m, wd, epochs, m_lr, first_cycle_steps, cycle_mult, warmup_steps, gamma):\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=m, weight_decay=wd)\n",
        "    if type == 'cosine_anneal':\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs + 1, eta_min=m_lr)\n",
        "    elif type == 'cosine_anneal_wr':\n",
        "        scheduler = CosineAnnealingWarmupRestarts(optimizer,\n",
        "                                                  first_cycle_steps=first_cycle_steps,\n",
        "                                                  cycle_mult=cycle_mult,\n",
        "                                                  max_lr=lr,\n",
        "                                                  min_lr=m_lr,\n",
        "                                                  warmup_steps=warmup_steps,\n",
        "                                                  gamma=gamma)\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def train_valid(model, train_queue, optimizer, paths, validation_queue, decay, criterion=nn.CrossEntropyLoss()):\n",
        "    # initializing average per epoch metrics\n",
        "    train_loss = 0 #average train loss\n",
        "    validation_loss = 0 #average validation loss\n",
        "    train_accuracy = 0 #average train accuracy\n",
        "    validation_accuracy = 0 #average validation accuracy\n",
        "    # iterate over validation set\n",
        "    validation_iterator = iter(validation_queue)  # validation set iterator\n",
        "    training_iterator = iter(train_queue)  # train set iterator\n",
        "\n",
        "\n",
        "    for batch_idx, (train_inputs, train_targets) in enumerate(train_queue):\n",
        "        #print(batch_idx)\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "        # sample paths\n",
        "        # path_index, pool = sample_uniform(paths,3969)\n",
        "        path_index, pool = sample_uniform(paths,3969)\n",
        "        #calculate normalized probabilities\n",
        "        # counter_matrix[path_index, model_index] += 1\n",
        "        model.train()\n",
        "        model.set_path(pool) # setting path\n",
        "        train_inputs, train_targets = train_inputs.cuda(), train_targets.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        train_outputs = model(train_inputs)\n",
        "        train_minibatch_loss = criterion(train_outputs, train_targets)\n",
        "        train_minibatch_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += train_minibatch_loss.detach().cpu().item()\n",
        "        train_acc = calculate_accuracy(train_outputs, train_targets)\n",
        "        train_accuracy += train_acc\n",
        "        # validation\n",
        "        model.eval()\n",
        "        try:\n",
        "          validation_inputs, validation_targets = next(validation_iterator)\n",
        "        except:\n",
        "          validation_iterator = iter(validation_queue)\n",
        "          validation_inputs, validation_targets = next(validation_iterator)\n",
        "        validation_inputs, validation_targets = validation_inputs.cuda(), validation_targets.cuda()\n",
        "        validation_outputs = model(validation_inputs)\n",
        "        validation_minibatch_loss = criterion(validation_outputs, validation_targets)\n",
        "\n",
        "        # update weight matrix\n",
        "        valid_acc = copy.deepcopy(calculate_accuracy(validation_outputs, validation_targets))\n",
        "        valid_acc_batch = copy.deepcopy(valid_acc[0] / valid_acc[1])\n",
        "        validation_loss += validation_minibatch_loss.detach().cpu().item()\n",
        "        validation_accuracy += valid_acc\n",
        "\n",
        "        # #sanity checks\n",
        "        # print('selected path:', path_index, path)\n",
        "        # print('selected model:', model_index)\n",
        "        # print('counter:', counter_matrix)\n",
        "        # print('weight_mat:', weight_mat)\n",
        "\n",
        "    return train_loss, validation_loss, train_accuracy, validation_accuracy\n",
        "\n",
        "\n",
        "def validate_all(model, paths, num_paths, validation_queue):\n",
        "    print('evaluating of all models on all paths....')\n",
        "    init_acc_mat = torch.zeros((num_paths)) # initialize matrix for accuracy\n",
        "    init_acc_mat_per_class = torch.zeros((num_paths, 10)) # initialize matrix for accuracy\n",
        "\n",
        "    model.eval()\n",
        "    for j in range(num_paths):\n",
        "        valid_accuracy_batch = 0\n",
        "        valid_accuracy_epoch = 0\n",
        "        per_class = 0\n",
        "        model.set_path(paths[j])\n",
        "\n",
        "        for batch_idx, (validation_inputs, validation_targets) in enumerate(validation_queue):\n",
        "            validation_inputs, validation_targets = validation_inputs.cuda(), validation_targets.cuda()\n",
        "            validation_outputs = model(validation_inputs)\n",
        "            valid_acc = calculate_accuracy(validation_outputs, validation_targets)\n",
        "            valid_accuracy = copy.deepcopy(valid_acc)\n",
        "            valid_accuracy_batch += valid_accuracy\n",
        "            per_class += accuracy_per_class(validation_outputs, validation_targets)\n",
        "        valid_acc_epoch = valid_accuracy_batch[0] / valid_accuracy_batch[1]\n",
        "        valid_acc_epoch_per_class = per_class[0,:] / per_class[1,:]\n",
        "        init_acc_mat[j] = copy.deepcopy(valid_acc_epoch)\n",
        "        init_acc_mat_per_class[j,:] = copy.deepcopy(valid_acc_epoch_per_class)\n",
        "    return init_acc_mat, init_acc_mat_per_class\n",
        "\n",
        "def output_batch(model, paths, num_paths, validation_inputs_batch, train_inputs_batch):\n",
        "    print('Calculate output matrix....')\n",
        "    # valid_batch_out = torch.zeros((num_paths, 10)) # valid output\n",
        "    # train_batch_out = torch.zeros((num_paths, 10)) # train output\n",
        "    train_out = torch.zeros((num_paths, train_inputs_batch.size(0), 10))\n",
        "    batch_mat_val = torch.zeros((num_paths, train_inputs_batch.size(0), 10))\n",
        "    model.eval()\n",
        "    for j in range(num_paths):\n",
        "        model.set_path(paths[j])\n",
        "\n",
        "        train_inputs_batch= train_inputs_batch.cuda()\n",
        "        train_outputs = model(train_inputs_batch)\n",
        "        train_out= copy.deepcopy(train_outputs.detach())\n",
        "\n",
        "        validation_inputs_batch= validation_inputs_batch.cuda()\n",
        "        validation_outputs = model(validation_inputs_batch)\n",
        "        batch_mat_val[j,...] = copy.deepcopy(validation_outputs.detach())\n",
        "\n",
        "    return train_out, batch_mat_val\n",
        "\n",
        "\n",
        "def accuracy_per_class(logits, target):\n",
        "    correct_class = torch.zeros(10)\n",
        "    total_class = torch.zeros(10)\n",
        "    _, test_predicted = logits.max(1)\n",
        "    for c in range(10):\n",
        "        total_class[c] = target.eq(c).sum().item()\n",
        "        correct_class[c] = (test_predicted.eq(target) * target.eq(c)).sum()\n",
        "    return torch.vstack((correct_class, total_class))\n",
        "\n",
        "\n",
        "\n",
        "def calculate_accuracy(logits, target):\n",
        "    _, test_predicted = logits.max(1)\n",
        "    batch_total = target.size(0)\n",
        "    batch_correct = test_predicted.eq(target).sum().item()\n",
        "    return torch.tensor([batch_correct, batch_total])\n",
        "\n"
      ],
      "metadata": {
        "id": "d2aqaVJif0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_model"
      ],
      "metadata": {
        "id": "QrssMpZpgH-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "candidate_OP = ['id', 'ir_3x3_t3', 'ir_5x5_t6']\n",
        "OPS = OrderedDict()\n",
        "OPS['id'] = lambda inp, oup, stride: Identity(inp=inp, oup=oup, stride=stride)\n",
        "OPS['ir_3x3_t3'] = lambda inp, oup, stride: InvertedResidual(inp=inp, oup=oup, t=3, stride=stride, k=3)\n",
        "OPS['ir_5x5_t6'] = lambda inp, oup, stride: InvertedResidual(inp=inp, oup=oup, t=6, stride=stride, k=5)\n",
        "\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, inp, oup, stride):\n",
        "        super(Identity, self).__init__()\n",
        "        if stride != 1 or inp != oup:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(inp, oup, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(oup, affine=False)#, track_running_stats=False),\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, t, k=3, activation=nn.ReLU, use_se=False, **kwargs):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.t = t\n",
        "        self.k = k\n",
        "        self.use_se = use_se\n",
        "        assert stride in [1, 2]\n",
        "        hidden_dim = round(inp * t)\n",
        "        if t == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, k, stride, padding=k//2, groups=hidden_dim,\n",
        "                              bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim, affine=False)#, track_running_stats=False),\n",
        "                activation(inplace=True),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup, affine=False)#, track_running_stats=False)\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim, affine=False)#, track_running_stats=False),\n",
        "                activation(inplace=True),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, k, stride, padding=k//2, groups=hidden_dim,\n",
        "                              bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim, affine=False)#, track_running_stats=False),\n",
        "                activation(inplace=True),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup, affine=False)#, track_running_stats=False),\n",
        "            )\n",
        "        self.use_shortcut = inp == oup and stride == 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_shortcut:\n",
        "            return self.conv(x) + x\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class supernet(nn.Module): # contains all possible architectures\n",
        "    def __init__(self, num_classes=10, stages=[2, 3, 3], init_channels=32):\n",
        "        super(supernet, self).__init__()\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, init_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(init_channels, affine=False)#, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        ops_layers = []\n",
        "\n",
        "        channels = init_channels\n",
        "        for stage in stages:\n",
        "            for idx in range(stage):\n",
        "                ops_per_layer = []\n",
        "                for o in candidate_OP:\n",
        "                    op_func = OPS[o] # operation\n",
        "                    if idx == 0:\n",
        "                        # stride = 2\n",
        "                        ops_per_layer.append(op_func(channels, channels*2, 2).cuda())\n",
        "                    else:\n",
        "                        ops_per_layer.append(op_func(channels, channels, 1).cuda())\n",
        "                if idx == 0:\n",
        "                    channels *= 2\n",
        "                ops_layers.append(ops_per_layer)\n",
        "\n",
        "\n",
        "        self.all_ops = ops_layers\n",
        "        self.chosen_ops = None\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(channels, 1280, kernel_size=1, bias=False, stride=1),\n",
        "            nn.BatchNorm2d(1280, affine=False)#, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "        self.classifier = nn.Linear(1280, num_classes, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        #print('stem done...')\n",
        "        x = self.chosen_ops(x)\n",
        "        x = self.out(x)\n",
        "        out = self.classifier(x.view(x.size(0), -1))\n",
        "        return out\n",
        "\n",
        "    def set_path(self,path): # list of op indices\n",
        "        path_ = path.copy()\n",
        "        chosen_ops = []\n",
        "        #print(len(self.all_ops))\n",
        "        for l in range(len(path)):\n",
        "            # print(l)\n",
        "            op_func = self.all_ops[l][path_.pop(0)]\n",
        "            chosen_ops.append(op_func)\n",
        "        self.chosen_ops = nn.Sequential(*chosen_ops)\n",
        "        #print('path set...', path)\n",
        "        #print('path set...', self.chosen_ops)\n",
        "\n",
        "    def feature_extractor(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extracts feature maps after the stem, after each operation in chosen_ops,\n",
        "        and after each operation in self.out.\n",
        "        Assumes set_path() has been successfully called.\n",
        "        \"\"\"\n",
        "        feature_maps: List[torch.Tensor] = []\n",
        "\n",
        "        # 1. Process stem\n",
        "        x = self.stem(x)\n",
        "        # To include features after stem: feature_maps.append(x.clone())\n",
        "\n",
        "        # 2. Process each operation in chosen_ops\n",
        "        # Assuming self.chosen_ops is not None and is an nn.Sequential\n",
        "        # as per the user's request to assume set_path() has been called.\n",
        "        temp_x_chosen_ops = x\n",
        "        for op_module in self.chosen_ops: # type: ignore # Assuming chosen_ops is nn.Sequential\n",
        "            temp_x_chosen_ops = op_module(temp_x_chosen_ops)\n",
        "            feature_maps.append(temp_x_chosen_ops.clone())\n",
        "        x = temp_x_chosen_ops # Update x to be the output of the chosen_ops sequence\n",
        "\n",
        "        # 3. Process each operation in self.out\n",
        "        # self.out is always an nn.Sequential by definition in __init__\n",
        "        temp_x_out = x\n",
        "        for out_layer in self.out:\n",
        "            temp_x_out = out_layer(temp_x_out)\n",
        "            feature_maps.append(temp_x_out.clone())\n",
        "\n",
        "        return feature_maps\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDDfYLoOgDKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "pOeGbnAzgPk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import copy\n",
        "import os\n",
        "from datetime import datetime\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# from create_model import supernet\n",
        "\n",
        "\n",
        "\n",
        "# from utils.NAS_trainer import create_optimizer, train_valid, validate_all, output_batch\n",
        "# from utils.data_loader import data_loader\n",
        "# from utils.search_space_design import create_search_space\n",
        "# from utils.utility_functions import string_to_list\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch Resnet multi model NAS Training')\n",
        "parser.add_argument('--batchsize', '-b', type=int, default=512, help='batch size')\n",
        "parser.add_argument('--test_name', '-tn', type=int, default=1000, help='test name for saving model')\n",
        "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
        "parser.add_argument('--epochs', '-e', type=int, default=200, help='epochs to train')\n",
        "parser.add_argument('--validation_percent', '-vp', type=float, default=0.5, help='percent of train data for validation')\n",
        "\n",
        "parser.add_argument('--learning_rate', '-lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--min_learning_rate', '-mlr', default=0.0001, type=float, help='min learning rate')\n",
        "parser.add_argument('--weight_momentum', '-wm', default=0.9, type=float, help='momentum')\n",
        "parser.add_argument('--weight_decay', '-wd', default=0.0001, type=float, help='weight decay')\n",
        "\n",
        "parser.add_argument('--sched_type', '-st', default='cosine_anneal', type=str, help='scheduler type, cosine annealing')\n",
        "parser.add_argument('--first_cycle_steps', '-fcs', type=int, default=100, help='first cycle epochs')\n",
        "parser.add_argument('--cycle_mult', '-cm', default=1.0, type=float, help='Cycle steps magnification')\n",
        "parser.add_argument('--warmup_steps', '-ws', type=int, default=0, help='Linear warmup step size')\n",
        "parser.add_argument('--gamma', '-gm', default=1.0, type=float, help='Decrease rate of max learning rate by cycle')\n",
        "\n",
        "parser.add_argument('--data_dir', '-dd', type=str, default='./data/', help='dataset directory')\n",
        "parser.add_argument('--workers', '-wr', type=int, default=0, help='number of workers to load data')\n",
        "\n",
        "parser.add_argument('--ema_decay', '-emd', default=0.9, type=float, help='exponential moving average decay')\n",
        "parser.add_argument('--init_logit', '-ilog', default=1.0, type=float, help='initial logits for path probabilitites')\n",
        "\n",
        "# parser.add_argument('--local_rank', '-lrank', type=int, default=0)\n",
        "# parser.add_argument('-ngpu', type=int, default=4)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    print('Supernet pre-training...')\n",
        "    print('Test: ', args.test_name)\n",
        "    print('-------------------')\n",
        "    print(args)\n",
        "    print('-------------------')\n",
        "    seed_np = int(np.random.randint(low=0, high=9999, size=None, dtype=int))\n",
        "    print('random seed is:', seed_np)\n",
        "    torch.manual_seed(seed_np)\n",
        "    np.random.seed(seed_np)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "    startTime = datetime.now()\n",
        "    epochs = args.epochs\n",
        "    decay = args.ema_decay\n",
        "    # optimizer parameters\n",
        "    lr = args.learning_rate\n",
        "    mlr = args.min_learning_rate\n",
        "    moment = args.weight_momentum\n",
        "    w_decay = args.weight_decay\n",
        "    # schaduler parameters\n",
        "    sched_type = args.sched_type\n",
        "    first_cycle_steps = args.first_cycle_steps\n",
        "    cycle_mult = args.cycle_mult\n",
        "    warmup_steps = args.warmup_steps\n",
        "    gamma = args.gamma\n",
        "\n",
        "    save_dir = './supernet_checkpoint/' + str(args.test_name) + 'supernet_macro_' + str(args.test_name) + '.t7'  # checkpoint save directory\n",
        "\n",
        "\n",
        "    # create network\n",
        "    ncat = 10 # for CIFAR\n",
        "    print('creating mobilenet model....', flush=True)\n",
        "    layers = 10\n",
        "    net = supernet()\n",
        "    net.cuda()\n",
        "    print('-------------------', flush=True)\n",
        "    print(net, flush=True)\n",
        "    #\n",
        "    # net.set_path([0, 1, 1, 0, 1, 2, 1, 2])\n",
        "    # print('-------------------')\n",
        "    # print(net)\n",
        "    # print('-------------------')\n",
        "\n",
        "    optimizer, scheduler = create_optimizer(sched_type, net, lr, moment, w_decay, epochs, mlr, first_cycle_steps, cycle_mult, warmup_steps, gamma)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()  # classification loss criterion\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "    current_epoch = 0\n",
        "\n",
        "    ###loading data\n",
        "    if path.exists(args.data_dir):\n",
        "        dataset_dir = args.data_dir\n",
        "    else:\n",
        "        dataset_dir = '~/Desktop/codes/multires/data/'\n",
        "\n",
        "    index = 0\n",
        "    train_loader, validation_loader, test_loader, indices, num_class = data_loader(args.validation_percent,\n",
        "                                                                                   args.batchsize,\n",
        "                                                                                   indices=index,\n",
        "                                                                                   dataset_dir=dataset_dir,\n",
        "                                                                                   workers=args.workers)\n",
        "\n",
        "    ### intialize paths\n",
        "    num_paths = 3969 # unique paths\n",
        "    # get unique paths\n",
        "    df = pd.read_csv('./mat_dir/nas-bench-macro_cifar10_unique.csv', dtype='str')\n",
        "    paths_string = df['arch']\n",
        "    paths_list = []\n",
        "    for path_string in paths_string:\n",
        "        path_list = [int(x) for x in path_string]\n",
        "        paths_list.append(path_list)\n",
        "\n",
        "    paths = tuple(paths_list)\n",
        "\n",
        "    t_loss = []\n",
        "    t_acc = []\n",
        "    v_loss = []\n",
        "    v_acc = []\n",
        "    acc_mat = [0 for i in range(epochs+1)]\n",
        "    acc_mat_per_class = [0 for i in range(epochs+1)]\n",
        "    # val_all_output_matrix = [0 for i in range(epochs+1)]\n",
        "    val_batch_output_matrix = [0 for i in range(epochs+1)]\n",
        "    train_batch_output_matrix = [0 for i in range(epochs+1)]\n",
        "\n",
        "    # Take first batch of train and validation for output matrix computation\n",
        "    validation_iterator = iter(validation_loader)  # validation set iterator\n",
        "    training_iterator = iter(train_loader)  # validation set iterator\n",
        "    validation_inputs_batch, validation_targets_batch = next(validation_iterator)  # check if stable\n",
        "    train_inputs_batch, train_targets_batch = next(training_iterator)  # check if stable\n",
        "    if not os.path.isdir('./supernet_checkpoint/test_' + str(args.test_name)+'/'):\n",
        "        os.makedirs('./supernet_checkpoint/test_' + str(args.test_name)+'/')\n",
        "    save_dir = './supernet_checkpoint/test_' + str(args.test_name) + '/supernet_chpt_' + str(\n",
        "        args.test_name) + '.t7'  # checkpoint save directory\n",
        "    for epoch in range(current_epoch, args.epochs + 1):\n",
        "        print('epoch ', epoch,flush=True)\n",
        "        print('net learning rate: ', optimizer.param_groups[0]['lr'], flush=True)\n",
        "        if (epoch % 50 == 0 or epoch == args.epochs):  # test and save checkpoint every 5 epochs\n",
        "            print('Calculating output for one batch...')\n",
        "            # one batch output matrix calculation for train and validation\n",
        "            _, val_out = output_batch(net, paths, num_paths, validation_inputs_batch, train_inputs_batch)\n",
        "            # print(val_out[0,0,:])\n",
        "            # print(val_out.size())\n",
        "            # train_batch_output_matrix[epoch] = copy.deepcopy(train_out.detach())\n",
        "            # val_batch_output_matrix[epoch] = copy.deepcopy(val_out.detach())\n",
        "            #save\n",
        "            output_save_dir = './supernet_checkpoint/test_' + str(args.test_name) + '/supernet_chpt_epoch_' + str(\n",
        "                epoch) + '.t7'  # checkpoint save directory\n",
        "            torch.save(copy.deepcopy(val_out.detach()), output_save_dir)\n",
        "            print('Calculating output for all validation set...', flush=True)\n",
        "            # if (epoch % 20 == 0 or epoch == args.epochs):\n",
        "            if 0:\n",
        "                init_acc_mat_per_class, accuracy_mat = validate_all(net, paths, num_paths, validation_loader)\n",
        "                acc_mat[epoch] = copy.deepcopy(accuracy_mat)\n",
        "                acc_mat_per_class[epoch] = copy.deepcopy(init_acc_mat_per_class)\n",
        "                # output_matrix[epoch] = copy.deepcopy(init_acc_mat_per_class)\n",
        "                # print(init_acc_mat)\n",
        "            print('Saving models and progress...', flush=True)\n",
        "            save_checkpoint(save_dir, net, optimizer, scheduler, epoch,t_acc, v_acc, t_loss, v_loss,  index,  acc_mat, acc_mat_per_class, val_batch_output_matrix)\n",
        "\n",
        "        # train\n",
        "        train_loss, validation_loss, t_accuracy, v_accuracy = train_valid(net, train_loader, optimizer, paths,\n",
        "                  validation_loader, decay, criterion=nn.CrossEntropyLoss())\n",
        "\n",
        "        t_loss.append(train_loss)\n",
        "        t_acc.append(t_accuracy[0]/t_accuracy[1])\n",
        "        v_loss.append(validation_loss)\n",
        "        v_acc.append(v_accuracy[0]/v_accuracy[1])\n",
        "        print('train  acc', t_accuracy[0]/t_accuracy[1], flush=True)\n",
        "        print('v acc', v_accuracy[0]/v_accuracy[1], flush=True)\n",
        "\n",
        "        scheduler.step()\n",
        "        print('Training time: ', datetime.now() - startTime, flush=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(save_dir, model, optimizer, scheduler, epoch,t_acc, v_acc, t_loss, v_loss, index, acc_mat, acc_mat_per_class, val_batch_output_matrix):\n",
        "    state = {\n",
        "        'test_properties': vars(args),\n",
        "        'seed': args.seed,\n",
        "        'indices': index,\n",
        "        't_loss': t_loss,\n",
        "        't_acc': t_acc,\n",
        "        'v_loss': v_loss,\n",
        "        'v_acc': v_acc,\n",
        "        'acc_mat': acc_mat,\n",
        "        'acc_mat_per_class': acc_mat_per_class,\n",
        "        'model': model.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'weight_optimizer': optimizer.state_dict(),\n",
        "        'scheduler_state': scheduler.state_dict(),\n",
        "        'val_batch_output_matrix': val_batch_output_matrix,\n",
        "    }\n",
        "    # if not os.path.isdir('checkpoint'):\n",
        "    #     os.mkdir('checkpoint')\n",
        "    torch.save(state, save_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n"
      ],
      "metadata": {
        "id": "PIrZDE4xgPEz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}